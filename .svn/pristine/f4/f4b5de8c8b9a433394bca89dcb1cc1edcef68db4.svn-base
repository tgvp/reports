\documentclass[a4paper,11pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage[T1]{fontenc}
\usepackage{fix-cm}
\usepackage{url}
\usepackage[Glenn]{fncychap}
\usepackage[hmargin={2.5cm}]{geometry}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{blindtext}
\usepackage{tabularx}
\usepackage{tabulary}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[all]{hypcap}
\usepackage{enumitem}
\usepackage{xspace}


\renewcommand\FmN[1]{}
%opening
\title{Ciência sem Fronteiras (Science Without Borders) - 1st Technical Report}

\begin{document}

\maketitle

\chapter{Introduction and PhD Project Context}

The research work performed in this project is closely aligned with the ALEXANDRIA 
and iCrawl projects at the L3S Research Center -  
Leibniz Universit\"at Hannover\footnote{The L3S Research Center: \url{www.L3S.de}}.  
%
The L3S Research Center, the host institution of the PhD project, 
is one of the leading institutions
focusing on basic and applied research in the field of Web Science.  
% 

The ALEXANDRIA project \footnote{The ALEXANDRIA project: \url{http://alexandria-project.eu/}} 
aims at creating a foundation for temporal retrieval, exploration and analytics in Web archives.
%
Web archives are a rich source of data since they reflect rapid evolution
of the digital world. The interest in using the data
stored within Web archives has been observed by researchers from different
areas, such as history, sociology and law \cite{icrawlRequirements}. For example, 
the discussion in \cite{Brugger14} indicates that Web archives are an important source for
communication and media history, as well as within historiography in general.
%
In the context of Web archives, unique Web data collections available within 
the ALEXANDRIA project represent a great opportunity to create new
methods to explore and analyse the large scale data within Web archives.
It also provides contacts with high level
research scientists that has been crucial to the development of the PhD project.
%
While ALEXANDRIA focuses on already existing data collections within Web archives,
the iCrawl project\footnote{\url{http://icrawl.l3s.uni-hannover.de/}} 
aims at creation of such collections efficiently on demand, 
using Web and Social Web data. In particular, the role of Social Media to achieve
better temporal and topical coherence of the resulting collections is in the  
main research focus of the project.

In this reporting period, the student worked on the development of novel semantic analytics techniques 
to enable efficient light-weight access to large scale Web and Social Web data collections 
available in the context of the ALEXANDRIA and the iCrawl projects. 
%
In the following sections, we present a detailed description regarding the PhD work
developed at the L3S during this period. 
Then we present the work plan for the second year. 
%
Table~\ref{tab:work} illustrates the detailed information about the PhD
candidate at the L3S Research Center.

\begin{table} [!htb]
\begin{center}
\begin{tabular}{|l|l|l}
\cline{1-2}
\multicolumn{1}{|c|}{Name} & \multicolumn{1}{c|}{Tarcísio Souza Costa} &  \\ 
\cline{1-2}
\multicolumn{1}{|c|}{Process number} & \multicolumn{1}{c|}{201836/2014-9} &  \\ 
\cline{1-2}
\multicolumn{1}{|c|}{Report number} & \multicolumn{1}{c|}{1} &  \\
\cline{1-2}
\multicolumn{1}{|c|}{Area} & \multicolumn{1}{c|}{Computer Science} &  \\ 
\cline{1-2}
\multicolumn{1}{|c|}{Sub-area} & \multicolumn{1}{c|}{Computer Science} &  \\ 
\cline{1-2}
\multicolumn{1}{|c|}{Institution} & \multicolumn{1}{c|}{Instituto Federal do
Maranhão - Brasil} & \\
\cline{1-2}
\multicolumn{1}{|c|}{Department} & \multicolumn{1}{c|}{Professional Education} &\\
\cline{1-2}
\multicolumn{1}{|c|}{Professional Address} & \multicolumn{1}{c|}{
Pacas Street 5, Pinheiro Campus, Pinheiro - Maranhão, Brasil} &
\\
\cline{1-2}
\multicolumn{1}{|c|}{E-mail} &
\multicolumn{1}{c|}{souza@l3s.de | tarcisio@ifma.edu.br} &\\
\cline{1-2}
\multicolumn{1}{|c|}{Research Project} &
\multicolumn{1}{c|}{ALEXANDRIA}
&
\\
\cline{1-2}
\multicolumn{1}{|c|}{Title} & \multicolumn{1}{|c|}{Master} \\
\cline{1-2}
\multicolumn{1}{|c|}{Supervisor} & \multicolumn{1}{|c|}{Prof. Dr. Wolfgang
Nejdl | nejdl@l3s.de} \\
\cline{1-2}
\multicolumn{1}{|c|}{Advisor/Mentor} & \multicolumn{1}{|c|}{Dr. Elena Demidova
| demidova@l3s.de}

\\
\cline{1-2}
\multicolumn{1}{|c|}{Period of Evaluation} &
 \multicolumn{1}{c|}{October, 2014$\rightarrow$July, 2015} \\
\cline{1-2}
\end{tabular}

\caption{Student description}
\end{center}
\label{tab:work}
\end{table}



\chapter{Description of Research Activities}

\section{Research and development activities}

In the first year of the PhD project, the student has been involved in some of the key projects
at L3S that focus on analytics of the Social Web and Web content,  
including the iCrawl and the ALEXANDRIA projects. 

\subsection{R\&D activities in the context of the iCrawl project}
In this time period, the goal of the iCrawl project was to study how 
Social Web signals can be used to support focused collection of fresh Web content 
relevant to the current events, such as an Ebola outbreak, or a military conflict in Ukraine, 
in an effective and efficient way. In iCrawl, Social Web signals, in this case Twitter messages,
have been used to guide the Web crawler towards the newly created pages \cite{gossen15}.
%
In this initial phase, the student studied the literature related to the 
problems of focused crawling and Social Web analytics.
He also became familiar with the software developed in the context of the iCrawl project as well as 
with the Big Data research infrastructure of the L3S Research Center. 
Also in this period, the student started to learn new Big Data technologies, 
such as Hadoop MapReduce, Hive and Pig Latin. 

Large scale Web data collections, like those created in the iCrawl project, 
or, more generally, data contained within Web archives, require efficient access techniques
that can enable to efficiently locate relevant Social Web and Web content stored  
within the archive to enable further research. 
%
The goal of providing efficient light-weight access to large scale Web collections
lead the student to joining the ALEXANDRIA project at L3S later on as this project 
explicitly focuses on providing efficient entity-centric access to large scale 
Web collections. 
% 
An important question on the interconnection of the both projects
is how to use current Web to discover resources within Web archives.

\textbf{A URL discovery algorithm:} 
In order to provide efficient URL-based access to the Web archive data in the ALEXANDRIA 
context, the student developed a URL discovery algorithm that
maps a set of URLs from the current Web to the corresponding URLs from the Web archive. 
In this period the student investigated several methods to optimize the URL discovery 
algorithm using Big Data technologies, such as the join optimization using Hive tables and an integration with Apache HBase.

\subsection{R\&D activities in the context of the ALEXANDRIA project}
The student became familiar with the work plan of the ALEXANDRIA project and joined Work Package 
1 (Evolution-Aware Entity-Based Enrichment and Indexing), 
where he focused on creation of novel efficient light-weight methods for 
entity-centric access to Web archives. In this context, he 
became familiar with the state-of-the-art methods and tools in the field of 
Named Entity Recognition (NER), and other information extraction tools. 

\textbf{A light-weight full-text URL-based index:}
As full-text indexing of large scale Web archive data is computationally very expensive,
the student developed a light-weight full-text index that only takes the URLs into account.
To this extent, the student has written an analyzer for correct pre-processing 
and parsing of URLs to detect keywords. 
%
At the same period the student also learned a distributed indexing technology, named Elastic
Search. The index was
developed and made available in Kibana (a user-friendly Elastic Search graphical user
interface). 
%
The student developed categorization of 
URL search results by domain and mime type, performed index optimization such as setting some fields as analyzed 
by the search engine as well as refinement of search engine results to retrieve documents in certain 
periods of time. 
%
The index was further improved by inserting mime type, domain and
improvements related to temporal expressions automatically 
extracted from the URLs, also changes in the main
structure were performed. This index should be used as a starting point for the iCrawl
to find relevant documents within the archive. 

\textbf{Named entity extraction from URLs:}
Further semantic analytics of the URLs included named entity extraction from URLs. 
The paper describing this work was accepted for publication at the 
1st International Keystone Conference
\footnote{\url{http://www.keystone-cost.eu/ikc2015/about.php}} at the beginning of
July 2015. 
% lead to a publication 
% work on this dataset The student is the main author of the paper 
% % started writing a paper together with the other colleagues from
% % the iCrawl project, 
In this paper, the student analysed a subset of URLs, named as Popular German
Web. Such a subset is related to popular domains according to Alexa
ranking\footnote{\url{http://www.alexa.com}}. In this paper we measure the
quantity of URLs that are alive (the status code starts with ``2'') per year and per domain, 
also counting in which domains categories they are included, as Sports, Games,
Computes and so on. We also extracted temporal expressions
and entities using only the information within URLs.
%
We continue working on this topic to further improve the results, for instance by adding better
pre-processing and post-filtering procedures in the URLs before applying state
of the art techniques to extract entities, as Stanford Named Entity Recognition
(NER) \footnote{\url{http://nlp.stanford.edu/software/CRF-NER.shtml}}. Using
those procedures we reach a precision up to 85\% in our dataset. 
%
Further improvements such as evaluating
precision and filtering on a sample of distinct URLs, instead of captures and
presentation of a more detailed description of how the precision was measured are currently 
being performed by the student as a part of the camera-ready preparation
and to address the review comments.

\subsection{Artificial Bee Clustering Search (ABCS) algorithm}
In addition to the research activities and publications in the ALEXANDRIA project, during this reporting period 
the student 
received an acceptance message from an extended version of a paper to be published in
a journal, the Artificial Bee Clustering Search (ABCS) algorithm \cite{costaiwann13}. 
After the final version submitted, the journal paper was published \cite{costa2014}. 

\section{Integration in the L3S research environment}
At the beginning of the first year at the L3S Research Center, the student 
joined the team of the iCrawl project lead by Dr. Elena Demidova, who was appointed as the student advisor
by Prof. Wolfgang Nejdl. Later on, the student joined a larger group of researchers working on the 
ALEXANDRIA project.

A part of the student integration in the L3S research environment is the regular meetings with the 
advisor (once a week), and with Prof. Nejdl (once a month) 
as well as regular participation in the iCrawl and ALEXANDRIA project meetings.
These meetings are very helpful for the student to 
obtain feedback on his research work, in particular with respect to further improvement of the results
and promising research directions for future work.

During this reporting period, the student made two presentations of the research and development
results in the ALEXANDRIA meetings: The student presented the URL index using Kibana interface during
the ALEXANDRIA meeting on March 5th, 2015. The paper accepted for the 1st International Keystone Conference 
was presented by the student on June 18th, 2015.
%
These presentations provided valuable feedback for the student work from a larger group
of the L3S researchers involved in the project.


\section{Community services}

The student worked as a sub-reviewer under supervision of Dr. Thomas Risse for the 
Joint Conference on Digital Libraries 2015 (JCDL 2015). That was a good experience since
the paper student reviewed was related to the PhD work and was
very useful to improve the student knowledge in the field.

\section{Courses}

In winter semester, the student followed the course ``Foundations of Information Retrieval"
including lectures and exercises offered by Prof. Nejdl. This lecture provides a solid 
foundation to better understand Information Retrieval concepts and terminology
used in the context of the ALEXANDRIA project and also to become familiar with the  
typical evaluation methods. 

In summer semester, the student participated in the course 
``Advanced Methods of Information Retrieval'' conducted by Dr. Elena Demidova. In this course
Information Retrieval specific methods were presented from the practical point of view, 
including implementation exercises and challenging research questions during the lectures.
This course was very useful for the student to become familiar with a diversity
of relevant topics in the field as well as open research directions relevant to the doctoral
study.

\section{Accepted publications}

\textbf{Title:} Semantic URL Analytics to Support Efficient Annotation of Large Scale Web Archives. \\
\textbf{Authors:} Tarcísio Souza, Elena Demidova, Thomas Risse, Helge Holzmann, Gerhard Gossen and Julian Szymanski. \\
\textbf{To appear in:} Proceedings of the 1st International Keystone Conference, Coimbra, Portugal, September 2015.\\
\textbf{Abstract:}
``Long-term Web archives comprise Web documents gathered over longer time periods and can easily reach hundreds of terabytes in size.
Semantic annotations such as named entities can facilitate intelligent access to the Web archive data. 
However, the annotation of the entire archive content on this scale is often infeasible. The most efficient way to access the documents within Web archives is provided through their URLs, which are typically stored in 
dedicated index files. The URLs of the archived Web documents can contain
semantic information and can offer an efficient way to obtain initial semantic annotations for the archived documents. 
In this paper, we analyse the applicability of semantic analysis techniques such as named entity extraction
to the URLs in a Web archive. We evaluate the precision of the named entity
extraction from the URLs in the Popular German Web dataset and analyse the
proportion of the archived URLs from 1,444 popular domains in the time interval from 2000 to 2012 to which these techniques are applicable.
Our results demonstrate that named entity recognition can be successfully applied to a large number of URLs in 
our Web archive and provide a good starting point to efficiently annotate large scale collections of Web documents.''



\chapter {Conclusion and Outlook}

During the first reporting period, the student worked on the problems 
of light-weight semantic annotation of large web collections in the 
context of the ALEXANDRIA and the iCrawl projects. The student learned 
state-of-the-art Big Data processing technologies,
developed novel approaches for Web data analytics, and published the results.

The first period at the L3S Research Center was a really rich experience, since the student had
an opportunity to study different research areas, learn new technologies as
well as working with top-level researchers.
%
During this period the student could find a well defined working approach
implemented by the L3S, where regular meetings with the supervisor (one per month)
and the advisor (one per week) were performed, which was very useful to
improve the student knowledge, also helping him to improve the PhD work that has
been developed in last months. 
%
% %
% The difficulties the student faced in the first
% time were related to the organization of the work, 
% in particular with respect to estimating the time needed to become proficient with the 
% current Big Data technologies and software.
% %
% It was a satisfactory period and the first developed work
% was just a starting point for what can be done in next periods.

In the second year, the student will continue research in the context of the ALEXANDRIA 
and the iCrawl projects. In particular, he will further develop and evaluate semantic analysis 
techniques for URLs, such as terms, named entities and 
temporal expressions extraction and correlation. Furthermore, he will develop novel methods to identify
interconnections between such semantic information spread across URLs
to enable light-weight URL-based sub-collection extraction of information related to events
from large unfocused Web data collections.
In this context the student will rely on machine learning and 
statistical techniques as well as soft computing - based algorithms. 

\chapter{Work plan for the second year}

The research plan for the second year takes into account the 
ideas from the initial work plan proposal, as well as experiences 
collected within the first year of PhD and the research environment at the L3S Research Center.
The planned research activities will facilitate a better integration of the student 
in the L3S research environment and foster collaborations with the L3S researchers,
in particular in the context of the ALEXANDRIA project.

\section{Motivation}
The amount of temporal Web data available within large scale Web archive collections, 
such as the dataset from the Internet Archive available in the context of the ALEXANDRIA project, has been 
constantly growing over the last decade. The existing Web archives offer a unique possibility to look into 
the past of the Web and attract an increasing interest of researchers in digital humanities.
Nevertheless, these researches are typically interested to study smaller, but focused event-centric 
sub-collections of documents contained in a Web archive. However, extraction of sub-collections 
is difficult given the size of the data. Therefore, it is important to develop 
light-weight efficient techniques that can help to locate relevant data within the archive.
In particular, in this work we would like to enable efficient access and sub-collection creation  
using metadata contained in the URLs of the archived pages using NLP techniques and 
soft computing algorithms.

%
% Typically, researchers working with the Web archives are interested in obtaining a
% topically coherent collections of documents that come from a specific time and
% contain information with respect to a specific topic. 


\section{Background}

\subsection{The iCrawl Software}
The previously mentioned work, the iCrawl software \cite{gossen15}, estimates the
freshness of web collections by integrating social media and a focused web
crawler. The crawler is automatically guided towards fresh content, which
a social media component, like twitter, support such process providing updated
content related to a certain topic as the Ukraine crisis. Such work represent a
novel approach, since current work consider topical and temporal aspects in
isolation and sometimes failing when trying to get not only relevant document
to a certain topic, but also up-to-date \cite{gossen15}. As mentioned in
this work, even though Twitter only allows 140 characters for each post, social media
platforms offer more expressive queries, which allows the crawler to retrieve
posts matching certain keywords relevant to a certain topic. The results
confirm that Twitter can be effectively used as a source of fresh content.

Following the work plan we described in last year and considering the iCrawl,
we are planing to develop soft computing-based approaches to be integrated to
iCrawl, which also contains a focused component. The approach in the current Web
could be used as entry points for the web archives to construct good sub-collections.

\subsection{Soft Computing} 
Soft computing (\textit{SC}) techniques constitute a large set of
methodologies as fuzzy logic, neural networks, probabilistic reasoning, evolutionary computing as well as
parts of machine learning theory and others, they are useful to solve problems
requiring some form of intelligence. The applicability of \textit{SC} is related
to its tolerance to imprecision, uncertainty, partial truth and approximation.
Evolutionary computing, as the genetic algorithms and its variants have been used in 
several forms in the literature of focused crawlers, such as in \cite{genetic1}, \cite{enhancedfocused} and \cite{genetic3}. 
Genetic Algorithms (GA) has an implicit parallelism, which is able to efficiently use global information from
the search space, but using them alone might reduce its capacity to
cover more promising areas of the search space, which is an
important factor to find the global optimal solution of many problems.
Therefore, hybrid approaches using genetic algorithms has been applied to
improve the exploitation characteristic of the GA, as in \cite{advances}.
%Therefore, we want to design and implement state-of-the-art
%\textit{SC} approaches in the ALEXANDRIA dataset combined with efficient local
%search procedures to remedy this problem.

According to \cite{focusedgenetic}, researchers found some structural 
properties of Web communities that made local searching algorithms were not suitable for focused crawling. 
Many Web pages could be missed by focused crawlers, due to their local searching
nature \cite{focusedgenetic}. In this way, evolutionary
algorithms as GA are promising methods to address the problem of focused crawler
described above, since they are going to across some vast search spaces
efficiently and can discover the approximate global optimal solutions instead of the local ones.
Therefore, we particularly want to design an hybrid evolutionary
algorithm (GA + clustering and local search procedures) to the web archives
search space, where local search procedures will be performed only in promising areas previously detected 
by an iterative clustering
process, as describe in \cite{advances} and \cite{costa2014}.
Many experiments performed using pure and hybrid evolutionary algorithms in
literature confirmed that the population converge to the best individual and
in the majority of cases hybrid approaches outperforms pure ones. 

\section{Research questions}

In the second year, the student will continue working on the 
efficient light-weight semantic access methods to data in Web archives. 
In particular, the student will work on the following research problems:

\begin{itemize}
\item [1.] Continue development and evaluation of techniques for analysis of named entities and 
temporal expressions in URLs.
\item [2.] Study the correlation between semantic information extracted from URLs and document content.
\item [3.] Develop methods to efficient URL-based creation of sub-collections from the archives
using, among others, soft computing - based algorithms. 
% \item [4.] Develop methods to efficient URL-based creation of sub-collections from the archives
% using, among others, soft computing-based algorithms. 
\end{itemize}

In the following, we describe these problems in more detail.

%\section{Research Problems}

\subsection{Semantic URL Analytics}
As mentioned above, semantic URL analytics can enable light-weight access to large scale data stored in 
Web archives. 
%
We are currently working on NER and time extraction improvements related
to some tasks that will be performed aiming to better analyse the NER evaluated
in the aforementioned paper. In general we performed a large scale data analysis
and pre-processing, which would be a preparation to the soft computing-based
algorithms that could be applied to the ALEXANDRIA dataset.

In the second year, we will further develop and evaluate techniques for 
URL analytics, including extraction of terms, named entities and temporal expressions. 
%
In particular, we will evaluate the results of the proposed temporal expression extraction 
against the state-of-the-art baselines, such as SUTime. Also, term extraction results will be 
evaluated against existing dictionaries and query logs.



\subsection{Correlation between the URL and the Text}
Although Web archive access based on the URL analytics results can be very efficient, 
it is based on the assumption that the terms, entities and temporal expressions mentioned on 
the URLs are representative with respect to the document content. 
%
In order to verify this assumption, we will perform an evaluation on the large-scale Web archive data.
In particular, we are interested to see if and in which part of the document (e.g. title, first paragraph) 
contain the terms identified in the URLs using our analysis methods. 

\subsection{Identify seed URLs Using semantic URL metadata}
In the previous steps we analysed and indexed the URLs to capture semantic information they contain.
In this step, we will focus on the question of how to help users to employ such semantic information 
in creation of an effective specification of the desired sub-collection. In particular, the users 
often do not know which keywords, entities and time intervals are the most useful 
to select the relevant documents based on the metadata contained in the URLs. 
%
To assist users in this task, we will create graphs that reflect relatedness 
of keywords an other semantic metadata contained in the archived URLs.

use a machine learning approach to expand the initial specification of the event 
provided by the user and identify further keywords and semantic metadata that best describe relevant documents 
within the archive. To achieve this goal we will evaluate various features, such as keyword frequency and selectivity 
in the seed documents, document titles, anchor texts, and use other 
features that appear promising based on the results of the correlation analysis.


\subsection{Generation of the focused Sub-collections based on the URLs}

In the previous steps we developed methods to identify keywords and semantic metadata that can best be used to 
query the URL-based index in the archive.
In this step, we will focus on the question of how to use such semantic information 
to obtain topically and temporally focused document collections.
To this extent, we are planning to use soft-computing algorithms.

There are many ways to explore the ALEXANDRIA web
archives using an approximation method. Initially we want to create an
efficient automatic generator of starting points (seed URLs) for a focused
crawler to create topically coherent collections.
In order to reach this we need to perform a kind of
keyword classification based on a topic. Such keywords would be extracted from
the URLs in the archive, for that we could use a semantic term analyser to
verify the similarity among different terms using a previously constructed
dictionary.

Once we detect terms in the first place,
the search for other related terms in the document will help us to increase
the set of useful topic keywords. We want to run
this method to a number of different topics (e.g. German elections, floods in
Europe and 2006 FIFA world cup). 
%To find similar terms to those found in the
%URL, the genetic algorithm would be useful to construct a set of terms (genes)
% representing a document (individual) in the web archive.

In the literature of focused crawlers, when GAs were applied to improve the
relevance of downloaded web pages, as in \cite{enhancedfocused}, only a pure GA
was used, which means that our approach appears as a promise way to
find relevant documents by designing a hybrid evolutionary
algorithm to the focused crawler iCrawl component. A possible implementation is
to represent a document in a collection as an individual in the population 
and the document relevance could be seen as its 
\textit{fitness}\footnote{The fitness of an individual in a genetic algorithm 
is its representative value in the population, the best individual has the best fitness.}.
Thus, since we can produce good individuals within the first generations of an evolutionary algorithm, 
certainly good sub-populations
(sub-collections) can be obtained from the first ones by applying classical
genetic operators (\textit{mutation} and \textit{crossover}) to the collection.
According to \cite{Gordon88}, documents are represented by an array of keywords and they will
evolve through natural selection and genetic operators. In such work the
results were generally proved to be the best string of keywords
describing the document.

\section{Outlook}

Summarizing the future work in L3S regarding the generation of topically coherent collections, 
we will implement the automatic generator described above, following
some predefined topics to create good seed URLs as well as the hybrid algorithm
\cite{advances} will be integrated to the focused iCrawl component.
We will also compare the results of our hybrid approach against classical
focused crawlers with and without pure genetic algorithms. We expect
that more relevant documents will be generated by the hybrid approach
creating good topically coherent collections.

%When extracting keywords, we could use a certain algorithm (e.g.
%ABCS \cite{costa2014}) that would group them into similar groups or clusters.
%Such clusters could be seen as topics, as the German elections mentioned above
%% and useful keywords related to this topic could be ``Angela Merkel'' or
% ``Wahl'', that
%means election in German. We also want to use the social media to get useful
%keywords for a certain topic, since such media is a useful source to get topic
%keywords, as analysed in \cite{gossen15}. 

% \subsection{Proposed algorithm}
% 
% Following we describe the key elements from our metaheuristic to be designed to
% the ALEXANDRIA web archives. The population (also know as chromossomes or
% individuals from GA) means the total number of documents (web pages), where the 
% \textit{fitness} function evaluates a certain individual inside the
% population, which means that we will use some well-known function or construct
% our own fitness function to evaluate each web page for a certain topic, based on our
% previous knowledge about the keywords extracted from each web page.
% \begin{itemize}
%   \item Initialization - First the archive crawler will start from a seed of URLs, 
%   such seeds could be generated by the automatic method describe above or by any other.
%   \item Fitness function - Can be cosine similarity or other relevant document
%   similarity function. The fitness can contains a previous analysis of a certain
%   path from a web page in the archive collection towards
%   relevant documents. The fitness function could also be proportional to the
%   quantity of relevant documents we can reach from a certain document. We could
%   develop a ``neighborhood function'' for each web page.
%   \item Clustering - The ABCS algorithm proposed also contains a clustering
%   process, which we can classify the web pages accordingly. 
% \end{itemize}  
% 
% The contribution of this work could be the application for our ALEXANDRIA
% data itself and also the addition of novel measures to our algorithms, as the
% neighborhood function. We can also think as a contribution the insertion of
% local search procedures to our metaheuristic applied to web documents, since in
% the current work this approach was never used. The archive genetic crawler could
% be a new promising version of a web crawler trying to create sub-collections from
% web archives, the genetic component could be also added to the iCrawl architecture,
% using the genetic algorithm trying to expand the search space through sub-spaces
% aiming to find more relevant documents.

%The second idea is to create a kind of learning crawler that from a set of
%example pages (training set) could learn
%Since the clustering process is a promise technique for obtaining related
%subgroups from a larger data set, another application for the archive crawler
%could be 
%\begin{itemize}
 % \item Read the time extraction related work.
  %\item Using
  %\textit{SUTime}\footnote{\url{http://nlp.stanford.edu/software/sutime.shtml}}
  %as a baseline for temporal extraction.
  %\item Re-run the experiments against distinct URLs, instead of captures.
  %\item Calculate precision and recall on a sample of distinct URLs for time
  %and entities extraction.
  %\item Construct a routine to check where those extracted entities
  %from NER can be found in the HTML content.
  %\item Count the number of documents where those entities were found
  %in the archive.
  %\item Analyse the correlation of dates in URLs with other dates in the
  %document.
  %\item investigate ways to learn date patterns in URLs, perhaps using machine
  %learning techniques.
%\end{itemize}

%The second task, \textit{Terms extraction in URLs}, aim to construct a routine
%to extract terms in URLs that belongs to a certain dictionary. This task is
%just a start point to a more complex task which can provide a way to easily
%find URLs in the archive by means of a set of keywords, where such keywords
%could belong to a certain topic as ``German Election''. One can think a topic
% as a cluster where similar keywords (related to a certain topic) would be
%assimilated by the closest cluster, in this way we want to use a classification
%algorithm, as that one created by the student in his master thesis
%\cite{costa2014}. \textit{Clustering algorithm adaptation} means that the
%previous implemented algorithm in \cite{costa2014} to solve different problems
% need to be designed to this new one. In the following task, \textit{URL Related
%Work}, we plan to read in more detail the works presented in the submitted
% paper and also other relevant papers to obtain a more concrete idea regarding URL features extraction.

%The task \textit{lectures} means that the student will attend to lectures
% aiming to improve his work. For the winter semester in 2015, the student would like to
%participate in temporal information retrieval. Regarding the following two
%tasks, which are related to an analysis of the whole archive, in principle we
%want to perform a similar analysis we did in the first paper, since now an
%indexing task of the whole archive is running and we expect it will finish
%before september. Once this index is finished we will be able to perform those
%two tasks.


%Such task Once this task is finished, we are able to extend that paper in
% August and submit this extension as a journal paper.
\onehalfspace
\bibliography{bibliografia}{}
\bibliographystyle{plain}
%\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

%\bibitem{talbi-book}
%\newblock El-Ghazali Talbi. Metaheuristics - From Design to Implementation. Wiley, 2009.


%\bibitem{ga}

%\newblock David E. Goldberg. Genetic Algorithms in Search, Optimization and Machine Learning. Addison-Wesley Publishing Company, Reading, Massachusetts, 1989.

%\bibitem{abc}

%\newblock Dervis Karaboga and Bahriye Akay. A comparative study of artificial bee colony algorithm. Applied Mathematics and Computation, 214(1):108–132, 2009.

%\bibitem{de}

%\newblock Kenneth Price and Rainer Storn. Differential evolution: A simple evolution strategy
%for fast optimization. Dr. Dobb’s Journal of Software Tools, 22(4):341–359, December 1997.

%\bibitem{cs}

%\newblock A.C.M de Oliveira and L.A.N Lorena. Detecting promising areas by evolutionary clustering search. 2004.

%\bibitem{abcs}

%\newblock Costa, T.S. ; Oliveira, A.C.M. New Clustering Search approaches applied to continuous domain optimization. IEEE Congress on Evolutionary Computation (CEC), p.3214, 2013, Cancun. (DOI: 10.1109/CEC.2013.6557963)

%\bibitem{bap}

%\newblock Barros, V.H. ; Costa, T. S.; Oliveira, A. C.M. ; Lorena, L.A.N. Model and heuristic for berth allocation in tidal bulk ports with stock level constraints. Computers \& Industrial Engineering, v. 60, p. 606-613, 2011. (DOI: http://dx.doi.org/10.1016/j.cie.2010.12.018) 

%\bibitem {eda}

%\newblock P. Larranaga and J. A. Lozano, Estimation of Distribution Algorithms: A New Tool for Evolutionary Computation. Norwell, MA: Kluwer, 2001.

%\bibitem {ieeecec}

%\newblock P. N. Suganthan, N. Hansen, J. J. Liang, K. Deb, Y.-P. Chen, A. Auger,
%and S. Tiwari, “Problem Definitions and Evaluation Criteria for the
%CEC 2005 Special Session on Real-Parameter Optimization,” Nanyang
%Technol. Univ., Singapore, IIT Kanpur, India, KanGAL Rep. 2005005,
%May 2005.

%\end{thebibliography}
\end{document}

